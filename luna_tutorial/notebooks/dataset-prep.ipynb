{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation Tutorial\n",
    "\n",
    "Welcome to the dataset preparation tutorial! In this notebook, we will download a toy data set for the tutorial and prepare the necessary metadata tables used for later analysis. Here are the steps we will review:\n",
    "\n",
    "1. Verify prerequisites (from setup.ipynb)\n",
    "2. Download data\n",
    "3. Build the proxy table\n",
    "4. Run regional annotation ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that that you have the software prerequisites including the LUNA_HOME environment variable, make sure you can run the following without errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T22:07:51.873208Z",
     "iopub.status.busy": "2021-11-16T22:07:51.872940Z",
     "iopub.status.idle": "2021-11-16T22:07:52.003112Z",
     "shell.execute_reply": "2021-11-16T22:07:52.002597Z",
     "shell.execute_reply.started": "2021-11-16T22:07:51.873185Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mclassifier\u001b[0m/  \u001b[01;34mconf\u001b[0m/  dockerfile  \u001b[01;34mimg\u001b[0m/  Makefile  \u001b[01;34mnotebooks\u001b[0m/  README.md\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_PYTHON=!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T21:27:28.601970Z",
     "iopub.status.busy": "2021-11-16T21:27:28.601656Z",
     "iopub.status.idle": "2021-11-16T21:27:31.615860Z",
     "shell.execute_reply": "2021-11-16T21:27:31.615227Z",
     "shell.execute_reply.started": "2021-11-16T21:27:28.601942Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.9\n",
      "openjdk version \"1.8.0_275\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_275-b01)\n",
      "OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)\n",
      "env: JAVA_HOME=/usr\n",
      "\n",
      "\n",
      "\n",
      "/usr\n",
      "env: LUNA_HOME=tutorial_sandbox\n",
      "tutorial_sandbox\n",
      "pyluna             0.0.3\n",
      "pyluna-common      0.0.3\n",
      "pyluna-core        0.0.3\n",
      "pyluna-pathology   0.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!python3 --version\n",
    "!java -version\n",
    "%env JAVA_HOME=/usr\n",
    "%env PYSPARK_PYTHON=/usr\n",
    "!echo $PYSPARK_PYTHON\n",
    "!echo $PYSPARK_DRIVER_PYTHON\n",
    "!echo $SPARK_HOME\n",
    "!echo $JAVA_HOME\n",
    "\n",
    "%env LUNA_HOME=tutorial_sandbox\n",
    "!echo $LUNA_HOME\n",
    "%pip list | grep luna\n",
    "import luna\n",
    "import luna.pathology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a new directory for your project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a project workspace for your data, models, and outputs to go for this tutorial. The manifest file contains lineage information about this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T21:31:22.073958Z",
     "iopub.status.busy": "2021-11-16T21:31:22.073665Z",
     "iopub.status.idle": "2021-11-16T21:31:22.973879Z",
     "shell.execute_reply": "2021-11-16T21:31:22.973321Z",
     "shell.execute_reply.started": "2021-11-16T21:31:22.073932Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-16 16:31:22,702 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /gpfs/mskmindhdp_emc/user/shared_data_folder/pashaa/docker/luna_tutorial/notebooks/data-processing.log (INFO)>]\n",
      "2021-11-16 16:31:22,703 - INFO - luna.common.config - loading config file tutorial_sandbox/conf/manifest.yaml\n",
      "2021-11-16 16:31:22,717 - INFO - root - config files copied to tutorial_sandbox/PRO_12-123\n",
      "2021-11-16 16:31:22,717 - INFO - root - Code block 'generate project folder' took: 0.014773540198802948s\n",
      "total 1\n",
      "-rw-r--r-- 1 pashaa pashaa 254 Nov 16 16:31 manifest.yaml\n"
     ]
    }
   ],
   "source": [
    "!python3 -m luna.project.generate --manifest_file tutorial_sandbox/conf/manifest.yaml\n",
    "!ls -l $LUNA_HOME/PRO_12-123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see a new directory called *PRO_12-123* with the manifest file in it. This will be your project workspace!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download data\n",
    "\n",
    "The data that you will be using for this tutorial is a set of 5 whole slide images of ovarian cancer H&E slides, available in the svs file format. Whole slide imaging refers to the scanning of conventional glass slides for research purposes; in this case, these are slides that oncologists have used for inspecting cancer samples! We will download these images from Synapse, a data warehouse used for digital research. \n",
    "\n",
    "We will now make a folder for your data and the toy data set in this new project workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T21:44:51.595969Z",
     "iopub.status.busy": "2021-11-16T21:44:51.595700Z",
     "iopub.status.idle": "2021-11-16T21:44:51.631748Z",
     "shell.execute_reply": "2021-11-16T21:44:51.631241Z",
     "shell.execute_reply.started": "2021-11-16T21:44:51.595946Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1\n",
      "drwxr-xr-x 3 pashaa pashaa 4096 Nov 16 16:35 data\n",
      "-rw-r--r-- 1 pashaa pashaa  254 Nov 16 16:31 manifest.yaml\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p $LUNA_HOME/PRO_12-123/data/toy_data_set\n",
    "ls -l $LUNA_HOME/PRO_12-123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the pathology slides for your toy data set on Synapse. First, you must navigate to the Synapse website (https://www.synapse.org/) and create an account if you do not already have one. Once your account is created, open the site, search for the project ID (syn25946167) in the righthand corner, click the \"Files\" tab, and download the tar.gz file as a file (not as a package). This process may take a while, as you will be downloading a little under 5 GB of data onto your machine. Once downloaded, expand the tar file, and then relocate the five svs files into the host data directory that is volume mounted into the container. Then copy the data into the toy_data_set sub-directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T21:45:04.804160Z",
     "iopub.status.busy": "2021-11-16T21:45:04.803900Z",
     "iopub.status.idle": "2021-11-16T21:45:04.821924Z",
     "shell.execute_reply": "2021-11-16T21:45:04.821415Z",
     "shell.execute_reply.started": "2021-11-16T21:45:04.804132Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2551028.svs\n",
      "2551129.svs\n",
      "2551389.svs\n",
      "2551531.svs\n",
      "2551571.svs\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls $LUNA_HOME/PRO_12-123/data/toy_data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the proxy table\n",
    "\n",
    "Now, we will run the Whole Slide Image (WSI) ETL to database the slides and build a metadata table called a proxy table that simply catalogs the raw data files. For reference, ETL stands for extract-transform-load; it is the method that often involves cleaning data, transforming data types, and loading data into different systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T22:00:47.547098Z",
     "iopub.status.busy": "2021-11-16T22:00:47.546847Z",
     "iopub.status.idle": "2021-11-16T22:00:59.373679Z",
     "shell.execute_reply": "2021-11-16T22:00:59.372924Z",
     "shell.execute_reply.started": "2021-11-16T22:00:47.547076Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- wsi_record_uuid: string (nullable = true)\n",
      " |-- slide_id: string (nullable = true)\n",
      " |-- metadata: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 17:00:48,346 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /gpfs/mskmindhdp_emc/user/shared_data_folder/pashaa/docker/luna_tutorial/notebooks/data-processing.log (INFO)>]\n",
      "2021-11-16 17:00:48,346 - INFO - root - data_ingestions_template: tutorial_sandbox/conf/wsi_config.yaml\n",
      "2021-11-16 17:00:48,346 - INFO - root - config_file: tutorial_sandbox/conf/app_config.yaml\n",
      "2021-11-16 17:00:48,346 - INFO - root - processes: ['delta']\n",
      "2021-11-16 17:00:48,346 - INFO - luna.common.config - loading config file tutorial_sandbox/conf/app_config.yaml\n",
      "2021-11-16 17:00:48,348 - INFO - luna.common.config - loading config file tutorial_sandbox/conf/wsi_config.yaml\n",
      "2021-11-16 17:00:48,355 - INFO - luna.common.config - validating config tutorial_sandbox/conf/wsi_config.yaml against schema /gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/luna/pathology/proxy_table/data_ingestion_template_schema.yml for DATA_CFG\n",
      "2021-11-16 17:00:48,385 - INFO - root - config files copied to tutorial_sandbox/PRO_12-123/configs/WSI_toy_data_set\n",
      "Warning: Ignoring non-Spark config property: fs.defaultFS\n",
      "Ivy Default Cache set to: /home/pashaa/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pashaa/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0bd493fc-1a10-45ca-986c-f6bebbc34e68;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;0.7.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 282ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;0.7.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0bd493fc-1a10-45ca-986c-f6bebbc34e68\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/6ms)\n",
      "21/11/16 17:00:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-11-16 17:00:53,161 - INFO - __main__ - generating binary proxy table...\n",
      "2021-11-16 17:00:53,190 - INFO - __main__ - Writing to tutorial_sandbox/PRO_12-123/tables/WSI_toy_data_set\n",
      "2021-11-16 17:00:55,138 - INFO - __main__ - Code block 'load wsi metadata' took: 1.948323793709278s\n",
      "21/11/16 17:00:58 ERROR Utils: Aborting task                        (0 + 5) / 5]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/11/16 17:00:58 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/11/16 17:00:58 ERROR Executor: Exception in task 4.0 in stage 0.0 (TID 4)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/11/16 17:00:58 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/11/16 17:00:58 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/11/16 17:00:58 ERROR FileFormatWriter: Job job_20211116170056_0000 aborted.\n",
      "21/11/16 17:00:58 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\n",
      "\t... 9 more\n",
      "21/11/16 17:00:58 WARN TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3, pllimsksparky2.mskcc.org, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/11/16 17:00:58 ERROR TaskSetManager: Task 3 in stage 0.0 failed 1 times; aborting job\n",
      "21/11/16 17:00:58 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, pllimsksparky2.mskcc.org, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\n",
      "\t... 9 more\n",
      "\n",
      "21/11/16 17:00:58 ERROR FileFormatWriter: Aborting job eb63bc82-cb29-4f06-a5fe-5bd76abb0327.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 1 times, most recent failure: Lost task 3.0 in stage 0.0 (TID 3, pllimsksparky2.mskcc.org, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:162)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:134)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:116)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:80)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:107)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:106)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:80)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:106)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:65)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:64)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:188)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:64)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:393)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "2021-11-16 17:00:58,596 - INFO - root - Code block 'generate proxy table' took: 10.249851930886507s\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/rh/rh-python36/root/usr/lib64/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/rh/rh-python36/root/usr/lib64/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/luna/pathology/proxy_table/generate.py\", line 236, in <module>\n",
      "    cli()\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/click/core.py\", line 829, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/click/core.py\", line 782, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/click/core.py\", line 1066, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/luna/pathology/proxy_table/generate.py\", line 142, in cli\n",
      "    exit_code = create_proxy_table()\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/luna/pathology/proxy_table/generate.py\", line 192, in create_proxy_table\n",
      "    df.coalesce(48).write.format(\"delta\").save(save_path)\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/pyspark/sql/readwriter.py\", line 827, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/pyspark/sql/utils.py\", line 128, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib64/python3.6/site-packages/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o271.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:162)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:134)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:116)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:80)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:107)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:106)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:80)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:106)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:65)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:64)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:188)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:64)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:152)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:393)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 1 times, most recent failure: Lost task 3.0 in stage 0.0 (TID 3, pllimsksparky2.mskcc.org, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n",
      "\t... 50 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/gpfs/mskmindhdp_emc/sw/env/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n",
      "    (\"%d.%d\" % sys.version_info[:2], version))\n",
      "Exception: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:260)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'python3 -m luna.pathology.proxy_table.generate \\\\\\n        -d $LUNA_HOME/conf/wsi_config.yaml \\\\\\n        -a $LUNA_HOME/conf/app_config.yaml \\\\\\n        -p delta\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1daf5ef70572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python3 -m luna.pathology.proxy_table.generate \\\\\\n        -d $LUNA_HOME/conf/wsi_config.yaml \\\\\\n        -a $LUNA_HOME/conf/app_config.yaml \\\\\\n        -p delta\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/gpfs/mskmindhdp_emc/user/shared_data_folder/pashaa/pt-venv/lib64/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2372\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/mskmindhdp_emc/user/shared_data_folder/pashaa/pt-venv/lib64/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/gpfs/mskmindhdp_emc/user/shared_data_folder/pashaa/pt-venv/lib64/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/mskmindhdp_emc/user/shared_data_folder/pashaa/pt-venv/lib64/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'python3 -m luna.pathology.proxy_table.generate \\\\\\n        -d $LUNA_HOME/conf/wsi_config.yaml \\\\\\n        -a $LUNA_HOME/conf/app_config.yaml \\\\\\n        -p delta\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m luna.pathology.proxy_table.generate \\\n",
    "        -d $LUNA_HOME/conf/wsi_config.yaml \\\n",
    "        -a $LUNA_HOME/conf/app_config.yaml \\\n",
    "        -p delta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step may take a while. At the end, your proxy table should be generated!\n",
    "\n",
    "Before we view the table, we must first update it to associate patient ID's with the slides. This is necessary for correctly training and validating the machine learning model in the coming notebooks. Once the slides are divided into \"tiles\" in the next notebook, the tiles are split between the training and validation sets for the ML model. If the tiles do not have patient ID's associated with them, then it is possible for tiles from one individual to appear in both the training and validation of the model; this would cause researchers to have an exaggerated interpretation of the model's accuracy, since we would essentially be validating the model on information that is too near to what it has already seen. \n",
    "\n",
    "Note that we will not be using patient IDs associated with MSK. Instead, we will be using spoof IDs that will suffice for this tutorial. When running this workflow with real data, make sure to include the IDs safely and securely. Run the following block of code to add a 'patient_id' column to the table and store it using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f6272da5ef0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# setup spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"test\") \\\n",
    "        .master('local[*]') \\\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.HDFSLogStore\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.dfs.client.use.datanode.hostname\", \"true\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .config(\"spark.executor.memory\", \"6g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(spark)\n",
    "\n",
    "# read WSI delta table\n",
    "wsi_table = spark.read.format(\"delta\") .load(\"file:////home/laluna/PRO_12-123/tables/WSI_toy_data_set\").toPandas()\n",
    "\n",
    "# insert spoof patient ids\n",
    "patient_id=[1,2,3,4,5]\n",
    "wsi_table['patient_id']=patient_id\n",
    "\n",
    "wsi_table\n",
    "\n",
    "# convert back to a spark table (update table)\n",
    "x = spark.createDataFrame(wsi_table)\n",
    "x.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"file:////home/laluna/PRO_12-123/tables/WSI_toy_data_set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the delta table down to a single layer so all data can be read as a parquet table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "wsi_table = DeltaTable.forPath(spark, \"file:////home/laluna/PRO_12-123/tables/WSI_toy_data_set\")  \n",
    "wsi_table.vacuum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we may view the WSI table! This table should have the metadata associated with the WSI slides that you just collected, including the patient IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>modificationTime</th>\n",
       "      <th>length</th>\n",
       "      <th>wsi_record_uuid</th>\n",
       "      <th>slide_id</th>\n",
       "      <th>metadata</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/home/laluna/PRO_12-123/data/toy_data_set...</td>\n",
       "      <td>2021-10-18 12:34:52.338</td>\n",
       "      <td>1413574341</td>\n",
       "      <td>WSI-03662b6be585f8bdb1a16a175a7cfda07c4057afe5...</td>\n",
       "      <td>2551571</td>\n",
       "      <td>{'aperio_StripeWidth': '2032', 'aperio_User': ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/home/laluna/PRO_12-123/data/toy_data_set...</td>\n",
       "      <td>2021-10-18 12:34:34.053</td>\n",
       "      <td>584611357</td>\n",
       "      <td>WSI-93ccfd50a210d0b8c7589352be9036ef5abf6b4f81...</td>\n",
       "      <td>2551129</td>\n",
       "      <td>{'aperio_StripeWidth': '2032', 'aperio_User': ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/home/laluna/PRO_12-123/data/toy_data_set...</td>\n",
       "      <td>2021-10-18 12:34:42.567</td>\n",
       "      <td>520642043</td>\n",
       "      <td>WSI-12677b7d98691d1eef8043727f27878eb9fda14b65...</td>\n",
       "      <td>2551531</td>\n",
       "      <td>{'aperio_StripeWidth': '2032', 'aperio_User': ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/home/laluna/PRO_12-123/data/toy_data_set...</td>\n",
       "      <td>2021-10-18 12:34:31.861</td>\n",
       "      <td>1322921471</td>\n",
       "      <td>WSI-1ba07f58166fc2073c854dd9b00a11eaca2203ff20...</td>\n",
       "      <td>2551028</td>\n",
       "      <td>{'aperio_Left': '12.423057', 'aperio_StripeWid...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/home/laluna/PRO_12-123/data/toy_data_set...</td>\n",
       "      <td>2021-10-18 12:34:40.668</td>\n",
       "      <td>966069709</td>\n",
       "      <td>WSI-f3890775a7f36c982aae28ac58de43b1852652fc20...</td>\n",
       "      <td>2551389</td>\n",
       "      <td>{'aperio_Left': '23.100784', 'aperio_StripeWid...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path        modificationTime  \\\n",
       "0  file:/home/laluna/PRO_12-123/data/toy_data_set... 2021-10-18 12:34:52.338   \n",
       "1  file:/home/laluna/PRO_12-123/data/toy_data_set... 2021-10-18 12:34:34.053   \n",
       "2  file:/home/laluna/PRO_12-123/data/toy_data_set... 2021-10-18 12:34:42.567   \n",
       "3  file:/home/laluna/PRO_12-123/data/toy_data_set... 2021-10-18 12:34:31.861   \n",
       "4  file:/home/laluna/PRO_12-123/data/toy_data_set... 2021-10-18 12:34:40.668   \n",
       "\n",
       "       length                                    wsi_record_uuid slide_id  \\\n",
       "0  1413574341  WSI-03662b6be585f8bdb1a16a175a7cfda07c4057afe5...  2551571   \n",
       "1   584611357  WSI-93ccfd50a210d0b8c7589352be9036ef5abf6b4f81...  2551129   \n",
       "2   520642043  WSI-12677b7d98691d1eef8043727f27878eb9fda14b65...  2551531   \n",
       "3  1322921471  WSI-1ba07f58166fc2073c854dd9b00a11eaca2203ff20...  2551028   \n",
       "4   966069709  WSI-f3890775a7f36c982aae28ac58de43b1852652fc20...  2551389   \n",
       "\n",
       "                                            metadata  patient_id  \n",
       "0  {'aperio_StripeWidth': '2032', 'aperio_User': ...           1  \n",
       "1  {'aperio_StripeWidth': '2032', 'aperio_User': ...           2  \n",
       "2  {'aperio_StripeWidth': '2032', 'aperio_User': ...           3  \n",
       "3  {'aperio_Left': '12.423057', 'aperio_StripeWid...           5  \n",
       "4  {'aperio_Left': '23.100784', 'aperio_StripeWid...           4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read WSI delta table\n",
    "wsi_table = spark.read.format(\"delta\") \\\n",
    "            .load(\"file:////home/laluna/PRO_12-123/tables/WSI_toy_data_set\").toPandas()\n",
    "\n",
    "# view table\n",
    "wsi_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the table is depicted above, congratulations, you  have successfully run the Whole Slide Image (WSI) ETL to database the slides!\n",
    "\n",
    "## Run the regional annotation ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole slide images that you downloaded are images of ovarian cancer, but not every pixel on each slide is a tumor. In fact, the images show tumor cells, normal ovarian cells, necrosis (dead cells), fibrosis (scarred cells), and more. Pathologists at Memorial Sloan Kettering examined each slide and denoted these different features by hand, providing us with regional annotations. You may think of regional annotations as scientific highlighter marks over the different regions of the image.\n",
    "\n",
    "What actually happens when the regional annotation ETL is run? First, annotation bitmaps are downloaded from SlideViewer, a repository which stores WSI images and their annotation data. These bitmaps are converted into numpy arrays, which are then converted into GeoJSON files and organized in the proxy table. The GeoJSON files store the annotation regions marked by pathologists as polygons, which makes the data simpler to store and analyze. Once the annotation files are loaded into QuPath- a software used for digital pathology- later in the pipeline, this data format becomes incredibly useful and easy to work with.\n",
    "\n",
    "To run the regional annotation ETL, try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building contours for label 1\n",
      "num_pixels with label 3612930\n",
      "num_contours 3\n",
      "[-1, -1, -1]\n",
      "No label 2 found\n",
      "Building contours for label 3\n",
      "num_pixels with label 20257170\n",
      "num_contours 3\n",
      "[-1, -1, -1]\n",
      "No label 4 found\n",
      "Building contours for label 5\n",
      "num_pixels with label 38403188\n",
      "num_contours 2\n",
      "[-1, -1]\n",
      "Building contours for label 6\n",
      "num_pixels with label 28809658\n",
      "num_contours 29\n",
      "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Building contours for label 7\n",
      "num_pixels with label 4451030\n",
      "num_contours 11\n",
      "[-1, -1, -1, -1, -1, 2, 2, -1, -1, -1, -1]\n",
      "Building contours for label 8\n",
      "num_pixels with label 1248\n",
      "num_contours 1\n",
      "[-1]\n",
      "No label 9 found\n",
      "No label 10 found\n",
      "No label 11 found\n",
      "No label 12 found\n",
      "No label 13 found\n",
      "No label 14 found\n",
      "No label 15 found\n",
      "Building contours for label 1\n",
      "num_pixels with label 16475072\n",
      "num_contours 5\n",
      "[-1, -1, -1, 2, 2]\n",
      "Building contours for label 2\n",
      "num_pixels with label 3750146\n",
      "num_contours 3\n",
      "[-1, 0, 0]\n",
      "Building contours for label 3\n",
      "num_pixels with label 115784442\n",
      "num_contours 11\n",
      "[-1, -1, 0, 1, 1, 1, -1, 1, -1, -1, 9]\n",
      "No label 4 found\n",
      "Building contours for label 5\n",
      "num_pixels with label 62971884\n",
      "num_contours 27\n",
      "[-1, 0, -1, -1, 0, -1, -1, 6, 0, -1, -1, 3, 3, 0, 0, 0, -1, 0, 0, 16, 16, 16, 16, 16, 16, 16, -1]\n",
      "No label 6 found\n",
      "No label 7 found\n",
      "No label 8 found\n",
      "No label 9 found\n",
      "No label 10 found\n",
      "No label 11 found\n",
      "No label 12 found\n",
      "No label 13 found\n",
      "No label 14 found\n",
      "No label 15 found\n",
      "Building contours for label 1\n",
      "num_pixels with label 394812\n",
      "num_contours 3\n",
      "[-1, -1, -1]\n",
      "No label 2 found\n",
      "Building contours for label 3\n",
      "num_pixels with label 880568\n",
      "num_contours 1\n",
      "[-1]\n",
      "Building contours for label 4\n",
      "num_pixels with label 255604\n",
      "num_contours 2\n",
      "[-1, -1]\n",
      "Building contours for label 5\n",
      "num_pixels with label 3428922\n",
      "num_contours 3\n",
      "[-1, -1, -1]\n",
      "No label 6 found\n",
      "No label 7 found\n",
      "No label 8 found\n",
      "No label 9 found\n",
      "Building contours for label 10\n",
      "num_pixels with label 2587508\n",
      "num_contours 8\n",
      "[-1, -1, -1, -1, -1, -1, 5, 5]\n",
      "No label 11 found\n",
      "Building contours for label 12\n",
      "num_pixels with label 11236\n",
      "num_contours 1\n",
      "[-1]\n",
      "No label 13 found\n",
      "No label 14 found\n",
      "No label 15 found\n",
      "No label 1 found\n",
      "Building contours for label 2\n",
      "num_pixels with label 344474790\n",
      "num_contours 2\n",
      "[-1, 0]\n",
      "No label 3 found\n",
      "Building contours for label 4\n",
      "num_pixels with label 62336170\n",
      "num_contours 3\n",
      "[-1, 0, 0]\n",
      "No label 5 found\n",
      "No label 6 found\n",
      "Building contours for label 7\n",
      "num_pixels with label 2720232\n",
      "num_contours 2\n",
      "[-1, -1]\n",
      "No label 8 found\n",
      "No label 9 found\n",
      "No label 10 found\n",
      "No label 11 found\n",
      "No label 12 found\n",
      "No label 13 found\n",
      "No label 14 found\n",
      "No label 15 found\n",
      "No label 1 found\n",
      "Building contours for label 2\n",
      "num_pixels with label 385750\n",
      "num_contours 1\n",
      "[-1]\n",
      "Building contours for label 3\n",
      "num_pixels with label 1205304\n",
      "num_contours 5\n",
      "[-1, -1, -1, -1, -1]\n",
      "Building contours for label 4\n",
      "num_pixels with label 6615826\n",
      "num_contours 11\n",
      "[-1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "No label 5 found\n",
      "Building contours for label 6\n",
      "num_pixels with label 51908714\n",
      "num_contours 8\n",
      "[-1, -1, -1, -1, -1, -1, 4, -1]\n",
      "Building contours for label 7\n",
      "num_pixels with label 3202104\n",
      "num_contours 2\n",
      "[-1, -1]\n",
      "No label 8 found\n",
      "No label 9 found\n",
      "No label 10 found\n",
      "No label 11 found\n",
      "No label 12 found\n",
      "No label 13 found\n",
      "No label 14 found\n",
      "No label 15 found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-18 12:45:34,278 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:34,279 - INFO - luna.common.config - loading config file PRO_12-123/my_conf/regional_annotation_config.yaml\n",
      "2021-10-18 12:45:34,286 - INFO - luna.common.config - loading config file PRO_12-123/my_conf/app_config.yaml\n",
      "2021-10-18 12:45:34,289 - INFO - root - data template: PRO_12-123/my_conf/regional_annotation_config.yaml\n",
      "2021-10-18 12:45:34,289 - INFO - root - config_file: PRO_12-123/my_conf/app_config.yaml\n",
      "2021-10-18 12:45:34,313 - INFO - root - config files copied to /home/laluna/PRO_12-123/configs/REGIONAL_METADATA_RESULTS\n",
      "2021-10-18 12:45:37,180 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,182 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,188 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,190 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,191 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,193 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,195 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,200 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,204 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,211 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,218 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,239 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,250 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,254 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,256 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,258 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,258 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,273 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,275 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,276 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,276 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,277 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,278 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,279 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,282 - INFO - root - FYI: Initalized logger, log file at: data-processing.log with handlers: [<StreamHandler <stderr> (INFO)>, <RotatingFileHandler /home/laluna/data-processing.log (INFO)>]\n",
      "2021-10-18 12:45:37,284 - INFO - __main__ - <Client: 'tcp://127.0.0.1:40535' processes=25 threads=25, memory=2.03 TB>\n",
      "2021-10-18 12:45:37,355 - INFO - __main__ - Table output directory = /home/laluna/PRO_12-123/tables/REGIONAL_METADATA_RESULTS\n",
      "2021-10-18 12:54:40,140 - INFO - luna.pathology.common.slideviewer_client - Unzipping ./computational oncology_tmp_zips/2019;HobS19-159147602774;2551129_ellensol.zip\n",
      "2021-10-18 12:54:40,159 - INFO - luna.pathology.common.slideviewer_client - Unzipping ./computational oncology_tmp_zips/2019;HobS19-409411851898;2551028_ellensol.zip\n",
      "2021-10-18 12:54:40,165 - INFO - luna.pathology.common.slideviewer_client - Unzipping ./computational oncology_tmp_zips/2019;HobS19-475053909405;2551389_soslowr.zip\n",
      "2021-10-18 12:54:46,312 - INFO - luna.pathology.common.annotation_utils - Added slide 2551389 to ./regional_bmps/2019_HobS19-475053909405_2551389/2551389_soslowr_annot.bmp  * * * *\n",
      "2021-10-18 12:54:49,373 - INFO - luna.pathology.common.annotation_utils -  >>>>>>> Processing [2551389] <<<<<<<<\n",
      "2021-10-18 12:54:49,374 - INFO - luna.common.config - loading config file /home/laluna/luna/conf/datastore.cfg\n",
      "2021-10-18 12:54:49,376 - INFO - luna.common.DataStore - Configured datastore with {'GRAPH_STORE_ENABLED': False, 'GRAPH_URI': 'neo4j://localhost:7687', 'GRAPH_USER': 'neo4j', 'GRAPH_PASSWORD': 'password', 'OBJECT_STORE_ENABLED': False, 'MINIO_URI': 'localhost:8001', 'MINIO_USER': 'minio', 'MINIO_PASSWORD': 'password', 'DOC_STORE_ENABLED': False, 'MONGODB_URI': 'mongodb://localhost:27017/'}\n",
      "2021-10-18 12:54:49,376 - INFO - luna.common.DataStore - Datstore file backend= ./slides\n",
      "2021-10-18 12:54:51,107 - INFO - luna.pathology.common.slideviewer_client - Unzipping ./computational oncology_tmp_zips/2019;HobS19-176164505079;2551531_soslowr.zip\n",
      "2021-10-18 12:54:56,505 - INFO - luna.pathology.common.slideviewer_client - Unzipping ./computational oncology_tmp_zips/2019;HobS19-030513574376;2551571_soslowr.zip\n",
      "2021-10-18 12:54:58,072 - INFO - luna.pathology.common.annotation_utils - Added slide 2551028 to ./regional_bmps/2019_HobS19-409411851898_2551028/2551028_ellensol_annot.bmp  * * * *\n",
      "2021-10-18 12:54:58,205 - INFO - luna.pathology.common.annotation_utils - Added slide 2551129 to ./regional_bmps/2019_HobS19-159147602774_2551129/2551129_ellensol_annot.bmp  * * * *\n",
      "2021-10-18 12:55:02,561 - INFO - luna.pathology.common.annotation_utils -  >>>>>>> Processing [2551028] <<<<<<<<\n",
      "2021-10-18 12:55:02,561 - INFO - luna.common.config - loading config file /home/laluna/luna/conf/datastore.cfg\n",
      "2021-10-18 12:55:02,563 - INFO - luna.common.DataStore - Configured datastore with {'GRAPH_STORE_ENABLED': False, 'GRAPH_URI': 'neo4j://localhost:7687', 'GRAPH_USER': 'neo4j', 'GRAPH_PASSWORD': 'password', 'OBJECT_STORE_ENABLED': False, 'MINIO_URI': 'localhost:8001', 'MINIO_USER': 'minio', 'MINIO_PASSWORD': 'password', 'DOC_STORE_ENABLED': False, 'MONGODB_URI': 'mongodb://localhost:27017/'}\n",
      "2021-10-18 12:55:02,564 - INFO - luna.common.DataStore - Datstore file backend= ./slides\n",
      "2021-10-18 12:55:02,781 - INFO - luna.pathology.common.annotation_utils -  >>>>>>> Processing [2551129] <<<<<<<<\n",
      "2021-10-18 12:55:02,781 - INFO - luna.common.config - loading config file /home/laluna/luna/conf/datastore.cfg\n",
      "2021-10-18 12:55:02,784 - INFO - luna.common.DataStore - Configured datastore with {'GRAPH_STORE_ENABLED': False, 'GRAPH_URI': 'neo4j://localhost:7687', 'GRAPH_USER': 'neo4j', 'GRAPH_PASSWORD': 'password', 'OBJECT_STORE_ENABLED': False, 'MINIO_URI': 'localhost:8001', 'MINIO_USER': 'minio', 'MINIO_PASSWORD': 'password', 'DOC_STORE_ENABLED': False, 'MONGODB_URI': 'mongodb://localhost:27017/'}\n",
      "2021-10-18 12:55:02,784 - INFO - luna.common.DataStore - Datstore file backend= ./slides\n",
      "2021-10-18 12:55:04,876 - INFO - luna.pathology.common.annotation_utils - Added slide 2551531 to ./regional_bmps/2019_HobS19-176164505079_2551531/2551531_soslowr_annot.bmp  * * * *\n",
      "2021-10-18 12:55:08,509 - INFO - luna.pathology.common.annotation_utils -  >>>>>>> Processing [2551531] <<<<<<<<\n",
      "2021-10-18 12:55:08,510 - INFO - luna.common.config - loading config file /home/laluna/luna/conf/datastore.cfg\n",
      "2021-10-18 12:55:08,512 - INFO - luna.common.DataStore - Configured datastore with {'GRAPH_STORE_ENABLED': False, 'GRAPH_URI': 'neo4j://localhost:7687', 'GRAPH_USER': 'neo4j', 'GRAPH_PASSWORD': 'password', 'OBJECT_STORE_ENABLED': False, 'MINIO_URI': 'localhost:8001', 'MINIO_USER': 'minio', 'MINIO_PASSWORD': 'password', 'DOC_STORE_ENABLED': False, 'MONGODB_URI': 'mongodb://localhost:27017/'}\n",
      "2021-10-18 12:55:08,512 - INFO - luna.common.DataStore - Datstore file backend= ./slides\n",
      "2021-10-18 12:55:14,263 - INFO - luna.common.DataStore - Save ./regional_npys/2019_HobS19-475053909405_2551389/2551389_soslowr_SVBMP-563af0ca040e3c949527d2ff68f3aea175910f23b8ee201f6a0a68dbcbd56905_annot.npy -> ./slides/2551389/soslowr/RegionalAnnotationBitmap/data\n",
      "2021-10-18 12:55:17,553 - INFO - luna.pathology.common.annotation_utils - Added slide 2551571 to ./regional_bmps/2019_HobS19-030513574376_2551571/2551571_soslowr_annot.bmp  * * * *\n",
      "2021-10-18 12:55:22,834 - INFO - luna.pathology.common.annotation_utils -  >>>>>>> Processing [2551571] <<<<<<<<\n",
      "2021-10-18 12:55:22,835 - INFO - luna.common.config - loading config file /home/laluna/luna/conf/datastore.cfg\n",
      "2021-10-18 12:55:22,837 - INFO - luna.common.DataStore - Configured datastore with {'GRAPH_STORE_ENABLED': False, 'GRAPH_URI': 'neo4j://localhost:7687', 'GRAPH_USER': 'neo4j', 'GRAPH_PASSWORD': 'password', 'OBJECT_STORE_ENABLED': False, 'MINIO_URI': 'localhost:8001', 'MINIO_USER': 'minio', 'MINIO_PASSWORD': 'password', 'DOC_STORE_ENABLED': False, 'MONGODB_URI': 'mongodb://localhost:27017/'}\n",
      "2021-10-18 12:55:22,837 - INFO - luna.common.DataStore - Datstore file backend= ./slides\n",
      "2021-10-18 12:55:53,369 - INFO - luna.common.DataStore - Save ./regional_npys/2019_HobS19-176164505079_2551531/2551531_soslowr_SVBMP-9c1c0d310437e5a9e0127afffd9c33727a06755b7108a64a4d50d63285208d3a_annot.npy -> ./slides/2551531/soslowr/RegionalAnnotationBitmap/data\n",
      "2021-10-18 12:55:58,313 - INFO - luna.common.DataStore - Save ./regional_npys/2019_HobS19-159147602774_2551129/2551129_ellensol_SVBMP-6ada14cb0ec4349691e2038902c5d03602f98cb4075520c336bb443a4290597e_annot.npy -> ./slides/2551129/ellensol/RegionalAnnotationBitmap/data\n",
      "2021-10-18 12:56:04,950 - INFO - luna.common.DataStore - Save ./regional_npys/2019_HobS19-409411851898_2551028/2551028_ellensol_SVBMP-d5604a6fe653249e71339d2e8d4a9595231bda52040a0edd2644061c18f9ef0b_annot.npy -> ./slides/2551028/ellensol/RegionalAnnotationBitmap/data\n",
      "2021-10-18 12:56:30,696 - INFO - luna.common.DataStore - Save ./regional_npys/2019_HobS19-030513574376_2551571/2551571_soslowr_SVBMP-d238bdf2aa671b3aa3780c57fc7c95ef12c18cca2f1b0ca1d77d63390f4507ea_annot.npy -> ./slides/2551571/soslowr/RegionalAnnotationBitmap/data\n",
      "2021-10-18 12:59:20,917 - INFO - luna.common.DataStore - Save -> ./slides/2551389/soslowr/RegionalAnnotationJSON/DEFAULT_LABELS\n",
      "2021-10-18 12:59:21,926 - INFO - luna.common.DataStore - Save -> ./slides/2551389/soslowr/RegionalAnnotationJSON/PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:22,820 - INFO - luna.common.DataStore - Save -> ./slides/2551389/soslowr/RegionalAnnotationJSON/OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:23,783 - INFO - luna.common.DataStore - Save -> ./slides/2551389/soslowr/RegionalAnnotationJSON/SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:26,792 - INFO - luna.common.DataStore - Save -> ./slides/2551389/CONCAT/RegionalAnnotationJSON/DEFAULT_LABELS\n",
      "2021-10-18 12:59:26,805 - INFO - luna.common.DataStore - Save -> ./slides/2551389/CONCAT/RegionalAnnotationJSON/PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:26,816 - INFO - luna.common.DataStore - Save -> ./slides/2551389/CONCAT/RegionalAnnotationJSON/OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:26,827 - INFO - luna.common.DataStore - Save -> ./slides/2551389/CONCAT/RegionalAnnotationJSON/SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:27,121 - INFO - __main__ -   sv_project_id  ...                            labelset\n",
      "2021-10-18 12:59:27,121 - INFO - __main__ - 0           134  ...                      DEFAULT_LABELS\n",
      "2021-10-18 12:59:27,121 - INFO - __main__ - 1           134  ...             PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:27,121 - INFO - __main__ - 2           134  ...            OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:27,121 - INFO - __main__ - 3           134  ...  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:27,121 - INFO - __main__ - 4           134  ...                      DEFAULT_LABELS\n",
      "2021-10-18 12:59:27,121 - INFO - __main__ - 5           134  ...             PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:27,121 - INFO - __main__ - 6           134  ...            OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:27,121 - INFO - __main__ - 7           134  ...  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 12:59:27,121 - INFO - __main__ - \n",
      "2021-10-18 12:59:27,121 - INFO - __main__ - [8 rows x 10 columns]\n",
      "2021-10-18 13:02:02,300 - INFO - luna.common.DataStore - Save -> ./slides/2551531/soslowr/RegionalAnnotationJSON/DEFAULT_LABELS\n",
      "2021-10-18 13:02:03,646 - INFO - luna.common.DataStore - Save -> ./slides/2551531/soslowr/RegionalAnnotationJSON/PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:04,859 - INFO - luna.common.DataStore - Save -> ./slides/2551531/soslowr/RegionalAnnotationJSON/OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:06,170 - INFO - luna.common.DataStore - Save -> ./slides/2551531/soslowr/RegionalAnnotationJSON/SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:10,082 - INFO - luna.common.DataStore - Save -> ./slides/2551531/CONCAT/RegionalAnnotationJSON/DEFAULT_LABELS\n",
      "2021-10-18 13:02:10,103 - INFO - luna.common.DataStore - Save -> ./slides/2551531/CONCAT/RegionalAnnotationJSON/PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:10,118 - INFO - luna.common.DataStore - Save -> ./slides/2551531/CONCAT/RegionalAnnotationJSON/OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:10,134 - INFO - luna.common.DataStore - Save -> ./slides/2551531/CONCAT/RegionalAnnotationJSON/SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:10,555 - INFO - __main__ -   sv_project_id  ...                            labelset\n",
      "2021-10-18 13:02:10,555 - INFO - __main__ - 0           134  ...                      DEFAULT_LABELS\n",
      "2021-10-18 13:02:10,555 - INFO - __main__ - 1           134  ...             PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:10,555 - INFO - __main__ - 2           134  ...            OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:10,555 - INFO - __main__ - 3           134  ...  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:10,555 - INFO - __main__ - 4           134  ...                      DEFAULT_LABELS\n",
      "2021-10-18 13:02:10,555 - INFO - __main__ - 5           134  ...             PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:10,555 - INFO - __main__ - 6           134  ...            OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:10,555 - INFO - __main__ - 7           134  ...  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:02:10,555 - INFO - __main__ - \n",
      "2021-10-18 13:02:10,555 - INFO - __main__ - [8 rows x 10 columns]\n",
      "2021-10-18 13:04:18,584 - INFO - luna.common.DataStore - Save -> ./slides/2551028/ellensol/RegionalAnnotationJSON/DEFAULT_LABELS\n",
      "2021-10-18 13:04:19,195 - INFO - luna.common.DataStore - Save -> ./slides/2551028/ellensol/RegionalAnnotationJSON/PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:19,950 - INFO - luna.common.DataStore - Save -> ./slides/2551028/ellensol/RegionalAnnotationJSON/OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:20,560 - INFO - luna.common.DataStore - Save -> ./slides/2551028/ellensol/RegionalAnnotationJSON/SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:21,991 - INFO - luna.common.DataStore - Save -> ./slides/2551028/CONCAT/RegionalAnnotationJSON/DEFAULT_LABELS\n",
      "2021-10-18 13:04:22,001 - INFO - luna.common.DataStore - Save -> ./slides/2551028/CONCAT/RegionalAnnotationJSON/PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:22,006 - INFO - luna.common.DataStore - Save -> ./slides/2551028/CONCAT/RegionalAnnotationJSON/OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:22,009 - INFO - luna.common.DataStore - Save -> ./slides/2551028/CONCAT/RegionalAnnotationJSON/SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:22,150 - INFO - __main__ -   sv_project_id  ...                            labelset\n",
      "2021-10-18 13:04:22,150 - INFO - __main__ - 0           134  ...                      DEFAULT_LABELS\n",
      "2021-10-18 13:04:22,150 - INFO - __main__ - 1           134  ...             PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:22,150 - INFO - __main__ - 2           134  ...            OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:22,150 - INFO - __main__ - 3           134  ...  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:22,150 - INFO - __main__ - 4           134  ...                      DEFAULT_LABELS\n",
      "2021-10-18 13:04:22,150 - INFO - __main__ - 5           134  ...             PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:22,150 - INFO - __main__ - 6           134  ...            OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:22,150 - INFO - __main__ - 7           134  ...  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:04:22,150 - INFO - __main__ - \n",
      "2021-10-18 13:04:22,150 - INFO - __main__ - [8 rows x 10 columns]\n",
      "2021-10-18 13:05:57,868 - INFO - luna.common.DataStore - Save -> ./slides/2551129/ellensol/RegionalAnnotationJSON/DEFAULT_LABELS\n",
      "2021-10-18 13:05:58,005 - INFO - luna.common.DataStore - Save -> ./slides/2551129/ellensol/RegionalAnnotationJSON/PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:58,258 - INFO - luna.common.DataStore - Save -> ./slides/2551129/ellensol/RegionalAnnotationJSON/OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:58,394 - INFO - luna.common.DataStore - Save -> ./slides/2551129/ellensol/RegionalAnnotationJSON/SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:58,956 - INFO - luna.common.DataStore - Save -> ./slides/2551129/CONCAT/RegionalAnnotationJSON/DEFAULT_LABELS\n",
      "2021-10-18 13:05:58,959 - INFO - luna.common.DataStore - Save -> ./slides/2551129/CONCAT/RegionalAnnotationJSON/PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:58,960 - INFO - luna.common.DataStore - Save -> ./slides/2551129/CONCAT/RegionalAnnotationJSON/OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:58,962 - INFO - luna.common.DataStore - Save -> ./slides/2551129/CONCAT/RegionalAnnotationJSON/SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:59,007 - INFO - __main__ -   sv_project_id  ...                            labelset\n",
      "2021-10-18 13:05:59,007 - INFO - __main__ - 0           134  ...                      DEFAULT_LABELS\n",
      "2021-10-18 13:05:59,007 - INFO - __main__ - 1           134  ...             PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:59,007 - INFO - __main__ - 2           134  ...            OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:59,007 - INFO - __main__ - 3           134  ...  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:59,007 - INFO - __main__ - 4           134  ...                      DEFAULT_LABELS\n",
      "2021-10-18 13:05:59,007 - INFO - __main__ - 5           134  ...             PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:59,007 - INFO - __main__ - 6           134  ...            OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:59,007 - INFO - __main__ - 7           134  ...  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:05:59,007 - INFO - __main__ - \n",
      "2021-10-18 13:05:59,007 - INFO - __main__ - [8 rows x 10 columns]\n",
      "2021-10-18 13:08:04,076 - INFO - luna.common.DataStore - Save -> ./slides/2551571/soslowr/RegionalAnnotationJSON/DEFAULT_LABELS\n",
      "2021-10-18 13:08:05,415 - INFO - luna.common.DataStore - Save -> ./slides/2551571/soslowr/RegionalAnnotationJSON/PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:06,475 - INFO - luna.common.DataStore - Save -> ./slides/2551571/soslowr/RegionalAnnotationJSON/OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:07,705 - INFO - luna.common.DataStore - Save -> ./slides/2551571/soslowr/RegionalAnnotationJSON/SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:10,131 - INFO - luna.common.DataStore - Save -> ./slides/2551571/CONCAT/RegionalAnnotationJSON/DEFAULT_LABELS\n",
      "2021-10-18 13:08:10,150 - INFO - luna.common.DataStore - Save -> ./slides/2551571/CONCAT/RegionalAnnotationJSON/PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:10,156 - INFO - luna.common.DataStore - Save -> ./slides/2551571/CONCAT/RegionalAnnotationJSON/OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:10,163 - INFO - luna.common.DataStore - Save -> ./slides/2551571/CONCAT/RegionalAnnotationJSON/SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:10,432 - INFO - __main__ -   sv_project_id  ...                            labelset\n",
      "2021-10-18 13:08:10,432 - INFO - __main__ - 0           134  ...                      DEFAULT_LABELS\n",
      "2021-10-18 13:08:10,432 - INFO - __main__ - 1           134  ...             PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:10,432 - INFO - __main__ - 2           134  ...            OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:10,432 - INFO - __main__ - 3           134  ...  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:10,432 - INFO - __main__ - 4           134  ...                      DEFAULT_LABELS\n",
      "2021-10-18 13:08:10,432 - INFO - __main__ - 5           134  ...             PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:10,432 - INFO - __main__ - 6           134  ...            OBJECT_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:10,432 - INFO - __main__ - 7           134  ...  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS\n",
      "2021-10-18 13:08:10,432 - INFO - __main__ - \n",
      "2021-10-18 13:08:10,432 - INFO - __main__ - [8 rows x 10 columns]\n",
      "2021-10-18 13:08:12,481 - INFO - root - Code block 'generate annotation geojson table' took: 1358.1938080079854s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 -m luna.pathology.refined_table.regional_annotation.dask_generate \\\n",
    "        -d PRO_12-123/my_conf/regional_annotation_config.yaml \\\n",
    "        -a PRO_12-123/my_conf/app_config.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the regional annotation ETL was correctly run, after the Jupyter cell finishes, you may load the regional annotations table! This table contains the metadata saved from running the ETL. It includes paths to the bitmap files, numpy files, and geoJSON files that were mentioned before. To load the table, run the following code cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sv_project_id</th>\n",
       "      <th>slideviewer_path</th>\n",
       "      <th>slide_id</th>\n",
       "      <th>user</th>\n",
       "      <th>bmp_filepath</th>\n",
       "      <th>npy_filepath</th>\n",
       "      <th>geojson_path</th>\n",
       "      <th>date</th>\n",
       "      <th>labelset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-409411851898;2551028.svs</td>\n",
       "      <td>2551028</td>\n",
       "      <td>ellensol</td>\n",
       "      <td>./regional_bmps/2019_HobS19-409411851898_25510...</td>\n",
       "      <td>./regional_npys/2019_HobS19-409411851898_25510...</td>\n",
       "      <td>./slides/2551028/ellensol/RegionalAnnotationJS...</td>\n",
       "      <td>2021-10-18 12:45:37.425520</td>\n",
       "      <td>DEFAULT_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-409411851898;2551028.svs</td>\n",
       "      <td>2551028</td>\n",
       "      <td>ellensol</td>\n",
       "      <td>./regional_bmps/2019_HobS19-409411851898_25510...</td>\n",
       "      <td>./regional_npys/2019_HobS19-409411851898_25510...</td>\n",
       "      <td>./slides/2551028/ellensol/RegionalAnnotationJS...</td>\n",
       "      <td>2021-10-18 12:45:37.425520</td>\n",
       "      <td>PIXEL_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-409411851898;2551028.svs</td>\n",
       "      <td>2551028</td>\n",
       "      <td>ellensol</td>\n",
       "      <td>./regional_bmps/2019_HobS19-409411851898_25510...</td>\n",
       "      <td>./regional_npys/2019_HobS19-409411851898_25510...</td>\n",
       "      <td>./slides/2551028/ellensol/RegionalAnnotationJS...</td>\n",
       "      <td>2021-10-18 12:45:37.425520</td>\n",
       "      <td>OBJECT_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-409411851898;2551028.svs</td>\n",
       "      <td>2551028</td>\n",
       "      <td>ellensol</td>\n",
       "      <td>./regional_bmps/2019_HobS19-409411851898_25510...</td>\n",
       "      <td>./regional_npys/2019_HobS19-409411851898_25510...</td>\n",
       "      <td>./slides/2551028/ellensol/RegionalAnnotationJS...</td>\n",
       "      <td>2021-10-18 12:45:37.425520</td>\n",
       "      <td>SIMPLIFIED_PIXEL_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-159147602774;2551129.svs</td>\n",
       "      <td>2551129</td>\n",
       "      <td>ellensol</td>\n",
       "      <td>./regional_bmps/2019_HobS19-159147602774_25511...</td>\n",
       "      <td>./regional_npys/2019_HobS19-159147602774_25511...</td>\n",
       "      <td>./slides/2551129/ellensol/RegionalAnnotationJS...</td>\n",
       "      <td>2021-10-18 12:45:37.425841</td>\n",
       "      <td>DEFAULT_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-159147602774;2551129.svs</td>\n",
       "      <td>2551129</td>\n",
       "      <td>ellensol</td>\n",
       "      <td>./regional_bmps/2019_HobS19-159147602774_25511...</td>\n",
       "      <td>./regional_npys/2019_HobS19-159147602774_25511...</td>\n",
       "      <td>./slides/2551129/ellensol/RegionalAnnotationJS...</td>\n",
       "      <td>2021-10-18 12:45:37.425841</td>\n",
       "      <td>PIXEL_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-159147602774;2551129.svs</td>\n",
       "      <td>2551129</td>\n",
       "      <td>ellensol</td>\n",
       "      <td>./regional_bmps/2019_HobS19-159147602774_25511...</td>\n",
       "      <td>./regional_npys/2019_HobS19-159147602774_25511...</td>\n",
       "      <td>./slides/2551129/ellensol/RegionalAnnotationJS...</td>\n",
       "      <td>2021-10-18 12:45:37.425841</td>\n",
       "      <td>OBJECT_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-159147602774;2551129.svs</td>\n",
       "      <td>2551129</td>\n",
       "      <td>ellensol</td>\n",
       "      <td>./regional_bmps/2019_HobS19-159147602774_25511...</td>\n",
       "      <td>./regional_npys/2019_HobS19-159147602774_25511...</td>\n",
       "      <td>./slides/2551129/ellensol/RegionalAnnotationJS...</td>\n",
       "      <td>2021-10-18 12:45:37.425841</td>\n",
       "      <td>SIMPLIFIED_PIXEL_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-475053909405;2551389.svs</td>\n",
       "      <td>2551389</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-475053909405_25513...</td>\n",
       "      <td>./regional_npys/2019_HobS19-475053909405_25513...</td>\n",
       "      <td>./slides/2551389/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425596</td>\n",
       "      <td>DEFAULT_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-475053909405;2551389.svs</td>\n",
       "      <td>2551389</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-475053909405_25513...</td>\n",
       "      <td>./regional_npys/2019_HobS19-475053909405_25513...</td>\n",
       "      <td>./slides/2551389/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425596</td>\n",
       "      <td>PIXEL_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-475053909405;2551389.svs</td>\n",
       "      <td>2551389</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-475053909405_25513...</td>\n",
       "      <td>./regional_npys/2019_HobS19-475053909405_25513...</td>\n",
       "      <td>./slides/2551389/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425596</td>\n",
       "      <td>OBJECT_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-475053909405;2551389.svs</td>\n",
       "      <td>2551389</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-475053909405_25513...</td>\n",
       "      <td>./regional_npys/2019_HobS19-475053909405_25513...</td>\n",
       "      <td>./slides/2551389/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425596</td>\n",
       "      <td>SIMPLIFIED_PIXEL_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-176164505079;2551531.svs</td>\n",
       "      <td>2551531</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-176164505079_25515...</td>\n",
       "      <td>./regional_npys/2019_HobS19-176164505079_25515...</td>\n",
       "      <td>./slides/2551531/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425419</td>\n",
       "      <td>DEFAULT_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-176164505079;2551531.svs</td>\n",
       "      <td>2551531</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-176164505079_25515...</td>\n",
       "      <td>./regional_npys/2019_HobS19-176164505079_25515...</td>\n",
       "      <td>./slides/2551531/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425419</td>\n",
       "      <td>PIXEL_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-176164505079;2551531.svs</td>\n",
       "      <td>2551531</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-176164505079_25515...</td>\n",
       "      <td>./regional_npys/2019_HobS19-176164505079_25515...</td>\n",
       "      <td>./slides/2551531/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425419</td>\n",
       "      <td>OBJECT_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-176164505079;2551531.svs</td>\n",
       "      <td>2551531</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-176164505079_25515...</td>\n",
       "      <td>./regional_npys/2019_HobS19-176164505079_25515...</td>\n",
       "      <td>./slides/2551531/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425419</td>\n",
       "      <td>SIMPLIFIED_PIXEL_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-030513574376;2551571.svs</td>\n",
       "      <td>2551571</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-030513574376_25515...</td>\n",
       "      <td>./regional_npys/2019_HobS19-030513574376_25515...</td>\n",
       "      <td>./slides/2551571/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425368</td>\n",
       "      <td>DEFAULT_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-030513574376;2551571.svs</td>\n",
       "      <td>2551571</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-030513574376_25515...</td>\n",
       "      <td>./regional_npys/2019_HobS19-030513574376_25515...</td>\n",
       "      <td>./slides/2551571/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425368</td>\n",
       "      <td>PIXEL_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-030513574376;2551571.svs</td>\n",
       "      <td>2551571</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-030513574376_25515...</td>\n",
       "      <td>./regional_npys/2019_HobS19-030513574376_25515...</td>\n",
       "      <td>./slides/2551571/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425368</td>\n",
       "      <td>OBJECT_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>134</td>\n",
       "      <td>2019;HobS19-030513574376;2551571.svs</td>\n",
       "      <td>2551571</td>\n",
       "      <td>soslowr</td>\n",
       "      <td>./regional_bmps/2019_HobS19-030513574376_25515...</td>\n",
       "      <td>./regional_npys/2019_HobS19-030513574376_25515...</td>\n",
       "      <td>./slides/2551571/soslowr/RegionalAnnotationJSO...</td>\n",
       "      <td>2021-10-18 12:45:37.425368</td>\n",
       "      <td>SIMPLIFIED_PIXEL_CLASSIFIER_LABELS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sv_project_id                      slideviewer_path slide_id      user  \\\n",
       "0            134  2019;HobS19-409411851898;2551028.svs  2551028  ellensol   \n",
       "1            134  2019;HobS19-409411851898;2551028.svs  2551028  ellensol   \n",
       "2            134  2019;HobS19-409411851898;2551028.svs  2551028  ellensol   \n",
       "3            134  2019;HobS19-409411851898;2551028.svs  2551028  ellensol   \n",
       "4            134  2019;HobS19-159147602774;2551129.svs  2551129  ellensol   \n",
       "5            134  2019;HobS19-159147602774;2551129.svs  2551129  ellensol   \n",
       "6            134  2019;HobS19-159147602774;2551129.svs  2551129  ellensol   \n",
       "7            134  2019;HobS19-159147602774;2551129.svs  2551129  ellensol   \n",
       "8            134  2019;HobS19-475053909405;2551389.svs  2551389   soslowr   \n",
       "9            134  2019;HobS19-475053909405;2551389.svs  2551389   soslowr   \n",
       "10           134  2019;HobS19-475053909405;2551389.svs  2551389   soslowr   \n",
       "11           134  2019;HobS19-475053909405;2551389.svs  2551389   soslowr   \n",
       "12           134  2019;HobS19-176164505079;2551531.svs  2551531   soslowr   \n",
       "13           134  2019;HobS19-176164505079;2551531.svs  2551531   soslowr   \n",
       "14           134  2019;HobS19-176164505079;2551531.svs  2551531   soslowr   \n",
       "15           134  2019;HobS19-176164505079;2551531.svs  2551531   soslowr   \n",
       "16           134  2019;HobS19-030513574376;2551571.svs  2551571   soslowr   \n",
       "17           134  2019;HobS19-030513574376;2551571.svs  2551571   soslowr   \n",
       "18           134  2019;HobS19-030513574376;2551571.svs  2551571   soslowr   \n",
       "19           134  2019;HobS19-030513574376;2551571.svs  2551571   soslowr   \n",
       "\n",
       "                                         bmp_filepath  \\\n",
       "0   ./regional_bmps/2019_HobS19-409411851898_25510...   \n",
       "1   ./regional_bmps/2019_HobS19-409411851898_25510...   \n",
       "2   ./regional_bmps/2019_HobS19-409411851898_25510...   \n",
       "3   ./regional_bmps/2019_HobS19-409411851898_25510...   \n",
       "4   ./regional_bmps/2019_HobS19-159147602774_25511...   \n",
       "5   ./regional_bmps/2019_HobS19-159147602774_25511...   \n",
       "6   ./regional_bmps/2019_HobS19-159147602774_25511...   \n",
       "7   ./regional_bmps/2019_HobS19-159147602774_25511...   \n",
       "8   ./regional_bmps/2019_HobS19-475053909405_25513...   \n",
       "9   ./regional_bmps/2019_HobS19-475053909405_25513...   \n",
       "10  ./regional_bmps/2019_HobS19-475053909405_25513...   \n",
       "11  ./regional_bmps/2019_HobS19-475053909405_25513...   \n",
       "12  ./regional_bmps/2019_HobS19-176164505079_25515...   \n",
       "13  ./regional_bmps/2019_HobS19-176164505079_25515...   \n",
       "14  ./regional_bmps/2019_HobS19-176164505079_25515...   \n",
       "15  ./regional_bmps/2019_HobS19-176164505079_25515...   \n",
       "16  ./regional_bmps/2019_HobS19-030513574376_25515...   \n",
       "17  ./regional_bmps/2019_HobS19-030513574376_25515...   \n",
       "18  ./regional_bmps/2019_HobS19-030513574376_25515...   \n",
       "19  ./regional_bmps/2019_HobS19-030513574376_25515...   \n",
       "\n",
       "                                         npy_filepath  \\\n",
       "0   ./regional_npys/2019_HobS19-409411851898_25510...   \n",
       "1   ./regional_npys/2019_HobS19-409411851898_25510...   \n",
       "2   ./regional_npys/2019_HobS19-409411851898_25510...   \n",
       "3   ./regional_npys/2019_HobS19-409411851898_25510...   \n",
       "4   ./regional_npys/2019_HobS19-159147602774_25511...   \n",
       "5   ./regional_npys/2019_HobS19-159147602774_25511...   \n",
       "6   ./regional_npys/2019_HobS19-159147602774_25511...   \n",
       "7   ./regional_npys/2019_HobS19-159147602774_25511...   \n",
       "8   ./regional_npys/2019_HobS19-475053909405_25513...   \n",
       "9   ./regional_npys/2019_HobS19-475053909405_25513...   \n",
       "10  ./regional_npys/2019_HobS19-475053909405_25513...   \n",
       "11  ./regional_npys/2019_HobS19-475053909405_25513...   \n",
       "12  ./regional_npys/2019_HobS19-176164505079_25515...   \n",
       "13  ./regional_npys/2019_HobS19-176164505079_25515...   \n",
       "14  ./regional_npys/2019_HobS19-176164505079_25515...   \n",
       "15  ./regional_npys/2019_HobS19-176164505079_25515...   \n",
       "16  ./regional_npys/2019_HobS19-030513574376_25515...   \n",
       "17  ./regional_npys/2019_HobS19-030513574376_25515...   \n",
       "18  ./regional_npys/2019_HobS19-030513574376_25515...   \n",
       "19  ./regional_npys/2019_HobS19-030513574376_25515...   \n",
       "\n",
       "                                         geojson_path  \\\n",
       "0   ./slides/2551028/ellensol/RegionalAnnotationJS...   \n",
       "1   ./slides/2551028/ellensol/RegionalAnnotationJS...   \n",
       "2   ./slides/2551028/ellensol/RegionalAnnotationJS...   \n",
       "3   ./slides/2551028/ellensol/RegionalAnnotationJS...   \n",
       "4   ./slides/2551129/ellensol/RegionalAnnotationJS...   \n",
       "5   ./slides/2551129/ellensol/RegionalAnnotationJS...   \n",
       "6   ./slides/2551129/ellensol/RegionalAnnotationJS...   \n",
       "7   ./slides/2551129/ellensol/RegionalAnnotationJS...   \n",
       "8   ./slides/2551389/soslowr/RegionalAnnotationJSO...   \n",
       "9   ./slides/2551389/soslowr/RegionalAnnotationJSO...   \n",
       "10  ./slides/2551389/soslowr/RegionalAnnotationJSO...   \n",
       "11  ./slides/2551389/soslowr/RegionalAnnotationJSO...   \n",
       "12  ./slides/2551531/soslowr/RegionalAnnotationJSO...   \n",
       "13  ./slides/2551531/soslowr/RegionalAnnotationJSO...   \n",
       "14  ./slides/2551531/soslowr/RegionalAnnotationJSO...   \n",
       "15  ./slides/2551531/soslowr/RegionalAnnotationJSO...   \n",
       "16  ./slides/2551571/soslowr/RegionalAnnotationJSO...   \n",
       "17  ./slides/2551571/soslowr/RegionalAnnotationJSO...   \n",
       "18  ./slides/2551571/soslowr/RegionalAnnotationJSO...   \n",
       "19  ./slides/2551571/soslowr/RegionalAnnotationJSO...   \n",
       "\n",
       "                         date                            labelset  \n",
       "0  2021-10-18 12:45:37.425520                      DEFAULT_LABELS  \n",
       "1  2021-10-18 12:45:37.425520             PIXEL_CLASSIFIER_LABELS  \n",
       "2  2021-10-18 12:45:37.425520            OBJECT_CLASSIFIER_LABELS  \n",
       "3  2021-10-18 12:45:37.425520  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS  \n",
       "4  2021-10-18 12:45:37.425841                      DEFAULT_LABELS  \n",
       "5  2021-10-18 12:45:37.425841             PIXEL_CLASSIFIER_LABELS  \n",
       "6  2021-10-18 12:45:37.425841            OBJECT_CLASSIFIER_LABELS  \n",
       "7  2021-10-18 12:45:37.425841  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS  \n",
       "8  2021-10-18 12:45:37.425596                      DEFAULT_LABELS  \n",
       "9  2021-10-18 12:45:37.425596             PIXEL_CLASSIFIER_LABELS  \n",
       "10 2021-10-18 12:45:37.425596            OBJECT_CLASSIFIER_LABELS  \n",
       "11 2021-10-18 12:45:37.425596  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS  \n",
       "12 2021-10-18 12:45:37.425419                      DEFAULT_LABELS  \n",
       "13 2021-10-18 12:45:37.425419             PIXEL_CLASSIFIER_LABELS  \n",
       "14 2021-10-18 12:45:37.425419            OBJECT_CLASSIFIER_LABELS  \n",
       "15 2021-10-18 12:45:37.425419  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS  \n",
       "16 2021-10-18 12:45:37.425368                      DEFAULT_LABELS  \n",
       "17 2021-10-18 12:45:37.425368             PIXEL_CLASSIFIER_LABELS  \n",
       "18 2021-10-18 12:45:37.425368            OBJECT_CLASSIFIER_LABELS  \n",
       "19 2021-10-18 12:45:37.425368  SIMPLIFIED_PIXEL_CLASSIFIER_LABELS  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyarrow.parquet import read_table\n",
    "\n",
    "regional_annotation_table = read_table(\"/home/laluna/PRO_12-123/tables/REGIONAL_METADATA_RESULTS\",\n",
    "                                      filters = [('user', '!=', f'CONCAT')]).to_pandas()\n",
    "regional_annotation_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have successfully set up your workspace, dowloaded the data, and run both the pathology and regional annotation ETLs to prepare your data. You are ready to move on to the tiling notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathology venv",
   "language": "python",
   "name": "pt-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
